{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933ad0f5",
   "metadata": {},
   "source": [
    "# VAE Hyperparameter Sweep (MLP vs Conv)\n",
    "\n",
    "This notebook runs a large hyperparameter sweep for both VAE architectures using the existing scripts:\n",
    "- `scripts/train_vae.py`\n",
    "- `scripts/eval_vae.py`\n",
    "\n",
    "It keeps the same dataset, chronological split logic, and evaluation process used in the main pipeline.\n",
    "\n",
    "## Goals\n",
    "1. Test many combinations (latent size, architecture widths, optimization knobs).\n",
    "2. Check if there is a clear best configuration.\n",
    "3. Quantify whether architecture choice (MLP vs Conv) materially changes outcomes after tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print('Python:', sys.version)\n",
    "print('Executable:', sys.executable)\n",
    "print('Notebook CWD:', Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Sweep Configuration\n",
    "# =========================\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    candidates = [start, *start.parents]\n",
    "    for p in candidates:\n",
    "        if (p / 'pyproject.toml').exists() and (p / 'scripts').exists():\n",
    "            return p\n",
    "    raise RuntimeError(f'Could not find project root from: {start}')\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PARQUET_PATH = (PROJECT_ROOT / 'data/processed/vae/parquet/AAPL_vsurf_processed.parquet').resolve()\n",
    "OUTPUT_ROOT = (PROJECT_ROOT / 'artifacts/tuning_sweep').resolve()\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Trial budget controls\n",
    "SEED = 42\n",
    "MAX_TRIALS_MLP = 60\n",
    "MAX_TRIALS_CONV = 60\n",
    "\n",
    "# Keep training protocol consistent with existing pipeline\n",
    "EPOCHS = 100\n",
    "PATIENCE = 20\n",
    "DEVICE = 'auto'\n",
    "\n",
    "# Optional cleanup after sweep\n",
    "KEEP_FULL_ARTIFACTS_FOR_TOP_K = 10\n",
    "\n",
    "assert PARQUET_PATH.exists(), f'Missing parquet: {PARQUET_PATH}'\n",
    "\n",
    "total_trials = MAX_TRIALS_MLP + MAX_TRIALS_CONV\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Parquet:', PARQUET_PATH)\n",
    "print('Output root:', OUTPUT_ROOT)\n",
    "print('Budget:', total_trials, 'trials total')\n",
    "print('Estimated runtime (~2 min/trial):', round(total_trials * 2 / 60, 2), 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d449b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Search Spaces\n",
    "# =========================\n",
    "\n",
    "# Shared knobs\n",
    "LR_SPACE = [3e-4, 7e-4, 1e-3, 2e-3]\n",
    "BETA_SPACE = [0.25, 0.5, 1.0, 2.0]\n",
    "BATCH_SPACE = [32, 64]\n",
    "WEIGHT_DECAY_SPACE = [0.0, 1e-6, 1e-5]\n",
    "\n",
    "# MLP-specific\n",
    "MLP_LATENT_SPACE = [4, 8, 12, 16, 24]\n",
    "MLP_HIDDEN_SPACE = [\n",
    "    [128, 64],\n",
    "    [256, 128],\n",
    "    [384, 192],\n",
    "    [512, 256],\n",
    "]\n",
    "\n",
    "# Conv-specific\n",
    "CONV_LATENT_SPACE = [4, 8, 12, 16, 24]\n",
    "CONV_CHANNEL_SPACE = [\n",
    "    [16, 32, 64],\n",
    "    [24, 48, 96],\n",
    "    [32, 64, 128],\n",
    "]\n",
    "CONV_FC_SPACE = [128, 256, 512]\n",
    "CONV_BATCHNORM_SPACE = [True, False]\n",
    "\n",
    "def build_mlp_grid():\n",
    "    grid = []\n",
    "    for latent_dim, hidden_dims, lr, beta, batch_size, wd in product(\n",
    "        MLP_LATENT_SPACE,\n",
    "        MLP_HIDDEN_SPACE,\n",
    "        LR_SPACE,\n",
    "        BETA_SPACE,\n",
    "        BATCH_SPACE,\n",
    "        WEIGHT_DECAY_SPACE,\n",
    "    ):\n",
    "        grid.append({\n",
    "            'model_type': 'mlp',\n",
    "            'latent_dim': latent_dim,\n",
    "            'hidden_dims': hidden_dims,\n",
    "            'lr': lr,\n",
    "            'beta': beta,\n",
    "            'batch_size': batch_size,\n",
    "            'weight_decay': wd,\n",
    "        })\n",
    "    return grid\n",
    "\n",
    "def build_conv_grid():\n",
    "    grid = []\n",
    "    for latent_dim, channels, fc_dim, batchnorm, lr, beta, batch_size, wd in product(\n",
    "        CONV_LATENT_SPACE,\n",
    "        CONV_CHANNEL_SPACE,\n",
    "        CONV_FC_SPACE,\n",
    "        CONV_BATCHNORM_SPACE,\n",
    "        LR_SPACE,\n",
    "        BETA_SPACE,\n",
    "        BATCH_SPACE,\n",
    "        WEIGHT_DECAY_SPACE,\n",
    "    ):\n",
    "        grid.append({\n",
    "            'model_type': 'conv',\n",
    "            'latent_dim': latent_dim,\n",
    "            'channels': channels,\n",
    "            'fc_dim': fc_dim,\n",
    "            'batchnorm': batchnorm,\n",
    "            'lr': lr,\n",
    "            'beta': beta,\n",
    "            'batch_size': batch_size,\n",
    "            'weight_decay': wd,\n",
    "        })\n",
    "    return grid\n",
    "\n",
    "mlp_grid = build_mlp_grid()\n",
    "conv_grid = build_conv_grid()\n",
    "\n",
    "print('MLP full grid size :', len(mlp_grid))\n",
    "print('Conv full grid size:', len(conv_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae36db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Deterministic Sampling\n",
    "# =========================\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "def sample_trials(grid, max_trials, rng):\n",
    "    if len(grid) <= max_trials:\n",
    "        return list(grid)\n",
    "    idx = list(range(len(grid)))\n",
    "    rng.shuffle(idx)\n",
    "    return [grid[i] for i in idx[:max_trials]]\n",
    "\n",
    "mlp_trials = sample_trials(mlp_grid, MAX_TRIALS_MLP, rng)\n",
    "conv_trials = sample_trials(conv_grid, MAX_TRIALS_CONV, rng)\n",
    "all_trials = mlp_trials + conv_trials\n",
    "rng.shuffle(all_trials)\n",
    "\n",
    "print('Selected MLP trials :', len(mlp_trials))\n",
    "print('Selected Conv trials:', len(conv_trials))\n",
    "print('Total selected      :', len(all_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11853990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Runner Utilities\n",
    "# =========================\n",
    "\n",
    "def trial_key(cfg):\n",
    "    payload = json.dumps(cfg, sort_keys=True)\n",
    "    return hashlib.md5(payload.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "def run_cmd_live(cmd, cwd, log_path, stage_name):\n",
    "    \"\"\"\n",
    "    Run command with live streaming output to notebook and file.\n",
    "    Returns (returncode, combined_output).\n",
    "    \"\"\"\n",
    "    env = dict(**os.environ)\n",
    "    env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "    env[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "\n",
    "    print(f\"\\n[{stage_name}] START\")\n",
    "    print(f\"[{stage_name}] CWD: {cwd}\")\n",
    "    print(f\"[{stage_name}] CMD: {' '.join(cmd)}\")\n",
    "\n",
    "    lines = []\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as lf:\n",
    "        proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            cwd=str(cwd),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            encoding=\"utf-8\",\n",
    "            errors=\"replace\",\n",
    "            bufsize=1,\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        last_heartbeat = time.time()\n",
    "        while True:\n",
    "            line = proc.stdout.readline() if proc.stdout is not None else \"\"\n",
    "            if line:\n",
    "                msg = line.rstrip(\"\\n\")\n",
    "                lines.append(msg)\n",
    "                lf.write(msg + \"\\n\")\n",
    "                print(f\"[{stage_name}] {msg}\")\n",
    "                last_heartbeat = time.time()\n",
    "            elif proc.poll() is not None:\n",
    "                break\n",
    "            else:\n",
    "                now = time.time()\n",
    "                if now - last_heartbeat > 20:\n",
    "                    print(f\"[{stage_name}] ... still running ...\")\n",
    "                    last_heartbeat = now\n",
    "                time.sleep(0.2)\n",
    "\n",
    "        returncode = proc.wait()\n",
    "\n",
    "    print(f\"[{stage_name}] END (code={returncode})\")\n",
    "    return returncode, \"\\n\".join(lines)\n",
    "\n",
    "def build_train_cmd(cfg, train_dir):\n",
    "    cmd = [\n",
    "        sys.executable, 'scripts/train_vae.py',\n",
    "        '--parquet', str(PARQUET_PATH),\n",
    "        '--model_type', cfg['model_type'],\n",
    "        '--latent_dim', str(cfg['latent_dim']),\n",
    "        '--epochs', str(EPOCHS),\n",
    "        '--batch_size', str(cfg['batch_size']),\n",
    "        '--lr', str(cfg['lr']),\n",
    "        '--beta', str(cfg['beta']),\n",
    "        '--weight_decay', str(cfg['weight_decay']),\n",
    "        '--patience', str(PATIENCE),\n",
    "        '--seed', str(SEED),\n",
    "        '--device', DEVICE,\n",
    "        '--output_dir', str(train_dir),\n",
    "    ]\n",
    "\n",
    "    if cfg['model_type'] == 'mlp':\n",
    "        cmd += ['--hidden_dims'] + [str(x) for x in cfg['hidden_dims']]\n",
    "    else:\n",
    "        cmd += ['--channels'] + [str(x) for x in cfg['channels']]\n",
    "        cmd += ['--fc_dim', str(cfg['fc_dim'])]\n",
    "        if not cfg['batchnorm']:\n",
    "            cmd += ['--no_batchnorm']\n",
    "\n",
    "    return cmd\n",
    "\n",
    "def build_eval_cmd(train_dir, eval_dir):\n",
    "    checkpoint = train_dir / 'vae_checkpoint.pt'\n",
    "    return [\n",
    "        sys.executable, 'scripts/eval_vae.py',\n",
    "        '--checkpoint', str(checkpoint),\n",
    "        '--parquet', str(PARQUET_PATH),\n",
    "        '--output_dir', str(eval_dir),\n",
    "        '--device', DEVICE,\n",
    "        '--n_plot_samples', '0',\n",
    "    ]\n",
    "\n",
    "def parse_metrics(eval_dir):\n",
    "    p = eval_dir / 'test_metrics.json'\n",
    "    if not p.exists():\n",
    "        return {}\n",
    "    return json.loads(p.read_text())\n",
    "\n",
    "def run_trial(cfg, trial_idx, total_trials):\n",
    "    key = trial_key(cfg)\n",
    "    trial_name = f\"{trial_idx:04d}_{cfg['model_type']}_{key}\"\n",
    "    trial_dir = OUTPUT_ROOT / trial_name\n",
    "    train_dir = trial_dir / 'train'\n",
    "    eval_dir = trial_dir / 'eval'\n",
    "    logs_dir = trial_dir / 'logs'\n",
    "\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_cmd = build_train_cmd(cfg, train_dir)\n",
    "    eval_cmd = build_eval_cmd(train_dir, eval_dir)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"TRIAL {trial_idx}/{total_trials} :: {trial_name}\")\n",
    "    print(f\"CONFIG :: {json.dumps(cfg, sort_keys=True)}\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_t0 = time.time()\n",
    "    tr_code, tr_combined = run_cmd_live(\n",
    "        train_cmd,\n",
    "        PROJECT_ROOT,\n",
    "        logs_dir / 'train_output.txt',\n",
    "        f\"{trial_name}::TRAIN\",\n",
    "    )\n",
    "    train_sec = time.time() - train_t0\n",
    "\n",
    "    result = {\n",
    "        'trial_idx': trial_idx,\n",
    "        'trial_name': trial_name,\n",
    "        'trial_key': key,\n",
    "        'model_type': cfg['model_type'],\n",
    "        'status': 'train_failed' if tr_code != 0 else 'train_ok',\n",
    "        'train_returncode': tr_code,\n",
    "        'eval_returncode': None,\n",
    "        'runtime_sec': None,\n",
    "        'train_runtime_sec': round(train_sec, 3),\n",
    "        'eval_runtime_sec': None,\n",
    "        'train_dir': str(train_dir),\n",
    "        'eval_dir': str(eval_dir),\n",
    "        **cfg,\n",
    "    }\n",
    "\n",
    "    if tr_code != 0:\n",
    "        (logs_dir / 'train_stderr.txt').write_text(tr_combined, encoding='utf-8')\n",
    "\n",
    "    if tr_code == 0:\n",
    "        eval_t0 = time.time()\n",
    "        ev_code, ev_combined = run_cmd_live(\n",
    "            eval_cmd,\n",
    "            PROJECT_ROOT,\n",
    "            logs_dir / 'eval_output.txt',\n",
    "            f\"{trial_name}::EVAL\",\n",
    "        )\n",
    "        eval_sec = time.time() - eval_t0\n",
    "\n",
    "        result['eval_returncode'] = ev_code\n",
    "        result['eval_runtime_sec'] = round(eval_sec, 3)\n",
    "        result['status'] = 'ok' if ev_code == 0 else 'eval_failed'\n",
    "\n",
    "        if ev_code != 0:\n",
    "            (logs_dir / 'eval_stderr.txt').write_text(ev_combined, encoding='utf-8')\n",
    "\n",
    "        if ev_code == 0:\n",
    "            metrics = parse_metrics(eval_dir)\n",
    "            result.update({\n",
    "                'elbo': metrics.get('elbo'),\n",
    "                'recon_loss': metrics.get('recon_loss'),\n",
    "                'kl_loss': metrics.get('kl_loss'),\n",
    "                'mse_original': metrics.get('mse_original'),\n",
    "                'mae_original': metrics.get('mae_original'),\n",
    "                'rmse_original': metrics.get('rmse_original'),\n",
    "                'n_samples': metrics.get('n_samples'),\n",
    "            })\n",
    "\n",
    "    result['runtime_sec'] = round(time.time() - t0, 3)\n",
    "\n",
    "    print(f\"[{trial_name}] STATUS={result['status']} | total={result['runtime_sec']:.1f}s | train={result['train_runtime_sec']:.1f}s | eval={result.get('eval_runtime_sec')}\")\n",
    "    if result.get('mae_original') is not None:\n",
    "        print(f\"[{trial_name}] MAE={result['mae_original']:.6f}, RMSE={result['rmse_original']:.6f}, ELBO={result['elbo']:.6f}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Execute Sweep (resume-safe)\n",
    "# =========================\n",
    "\n",
    "RESULTS_CSV = OUTPUT_ROOT / 'results.csv'\n",
    "\n",
    "if RESULTS_CSV.exists():\n",
    "    existing = pd.read_csv(RESULTS_CSV)\n",
    "    print(f'Loaded existing results: {len(existing)} rows')\n",
    "\n",
    "    # Keep only latest record per trial_key if duplicates exist\n",
    "    if 'trial_key' in existing.columns and not existing.empty:\n",
    "        existing = existing.sort_values('trial_idx').drop_duplicates(subset=['trial_key'], keep='last')\n",
    "\n",
    "    # Only successful trials are considered done\n",
    "    done_keys = set(existing.loc[existing['status'] == 'ok', 'trial_key'].astype(str).tolist())\n",
    "else:\n",
    "    existing = pd.DataFrame()\n",
    "    done_keys = set()\n",
    "    print('No existing results found; starting fresh.')\n",
    "\n",
    "pending = [cfg for cfg in all_trials if trial_key(cfg) not in done_keys]\n",
    "print(f'Pending trials: {len(pending)} / {len(all_trials)}')\n",
    "\n",
    "new_rows = []\n",
    "for i, cfg in enumerate(pending, start=1):\n",
    "    trial_idx = len(existing) + i\n",
    "    row = run_trial(cfg, trial_idx, len(all_trials))\n",
    "    new_rows.append(row)\n",
    "\n",
    "    # incremental checkpoint save\n",
    "    cur_df = pd.DataFrame(new_rows)\n",
    "    out_df = pd.concat([existing, cur_df], ignore_index=True)\n",
    "\n",
    "    # keep latest row per trial_key\n",
    "    if 'trial_key' in out_df.columns and not out_df.empty:\n",
    "        out_df = out_df.sort_values('trial_idx').drop_duplicates(subset=['trial_key'], keep='last')\n",
    "\n",
    "    out_df.to_csv(RESULTS_CSV, index=False)\n",
    "\n",
    "results = pd.read_csv(RESULTS_CSV) if RESULTS_CSV.exists() else pd.DataFrame(new_rows)\n",
    "print('Sweep complete. Rows in results:', len(results))\n",
    "print('Successful trials:', int((results['status'] == 'ok').sum()) if 'status' in results.columns else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Optional: prune non-top trial folders\n",
    "# =========================\n",
    "\n",
    "results = pd.read_csv(RESULTS_CSV)\n",
    "ok = results[results['status'] == 'ok'].copy()\n",
    "ok = ok.sort_values(['mae_original', 'rmse_original', 'elbo'], ascending=[True, True, True])\n",
    "keep_trials = set(ok.head(KEEP_FULL_ARTIFACTS_FOR_TOP_K)['trial_name'].tolist())\n",
    "\n",
    "deleted = 0\n",
    "for d in OUTPUT_ROOT.iterdir():\n",
    "    if not d.is_dir():\n",
    "        continue\n",
    "    if d.name in keep_trials:\n",
    "        continue\n",
    "    if d.name == '__pycache__':\n",
    "        continue\n",
    "    try:\n",
    "        shutil.rmtree(d)\n",
    "        deleted += 1\n",
    "    except Exception as e:\n",
    "        print('Could not delete', d, e)\n",
    "\n",
    "print(f'Removed {deleted} non-top trial directories. Kept top {len(keep_trials)} full artifacts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Analysis\n",
    "# =========================\n",
    "\n",
    "results = pd.read_csv(RESULTS_CSV)\n",
    "ok = results[results['status'] == 'ok'].copy()\n",
    "\n",
    "if ok.empty:\n",
    "    raise RuntimeError('No successful trials found.')\n",
    "\n",
    "ok['mae_vp'] = ok['mae_original'] * 100\n",
    "ok['rmse_vp'] = ok['rmse_original'] * 100\n",
    "ok['runtime_min'] = ok['runtime_sec'] / 60.0\n",
    "\n",
    "display(ok[['trial_name', 'model_type', 'mae_original', 'rmse_original', 'elbo', 'runtime_sec']].sort_values('mae_original').head(20))\n",
    "\n",
    "summary = ok.groupby('model_type').agg(\n",
    "    n_trials=('trial_name', 'count'),\n",
    "    best_mae=('mae_original', 'min'),\n",
    "    median_mae=('mae_original', 'median'),\n",
    "    best_rmse=('rmse_original', 'min'),\n",
    "    median_rmse=('rmse_original', 'median'),\n",
    "    mean_runtime_min=('runtime_min', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "print('Architecture summary:')\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c62a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for m, color in [('mlp', '#1f77b4'), ('conv', '#2ca02c')]:\n",
    "    vals = ok.loc[ok['model_type'] == m, 'mae_vp']\n",
    "    axes[0].hist(vals, bins=20, alpha=0.6, label=m.upper(), color=color)\n",
    "\n",
    "axes[0].set_title('MAE distribution (vol points)')\n",
    "axes[0].set_xlabel('MAE (vol points)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "\n",
    "for m, color in [('mlp', '#1f77b4'), ('conv', '#2ca02c')]:\n",
    "    vals = ok.loc[ok['model_type'] == m, 'rmse_vp']\n",
    "    axes[1].hist(vals, bins=20, alpha=0.6, label=m.upper(), color=color)\n",
    "\n",
    "axes[1].set_title('RMSE distribution (vol points)')\n",
    "axes[1].set_xlabel('RMSE (vol points)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared hyperparameter sensitivity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "\n",
    "colors = ok['model_type'].map({'mlp':'#1f77b4', 'conv':'#2ca02c'})\n",
    "\n",
    "axes[0,0].scatter(ok['lr'], ok['mae_vp'], c=colors, alpha=0.7)\n",
    "axes[0,0].set_xscale('log')\n",
    "axes[0,0].set_title('MAE vs Learning Rate')\n",
    "axes[0,0].set_xlabel('lr')\n",
    "axes[0,0].set_ylabel('MAE (vp)')\n",
    "\n",
    "axes[0,1].scatter(ok['beta'], ok['mae_vp'], c=colors, alpha=0.7)\n",
    "axes[0,1].set_title('MAE vs Beta')\n",
    "axes[0,1].set_xlabel('beta')\n",
    "axes[0,1].set_ylabel('MAE (vp)')\n",
    "\n",
    "ok.boxplot(column='mae_vp', by='batch_size', ax=axes[1,0])\n",
    "axes[1,0].set_title('MAE by Batch Size')\n",
    "axes[1,0].set_xlabel('batch_size')\n",
    "axes[1,0].set_ylabel('MAE (vp)')\n",
    "\n",
    "ok.boxplot(column='mae_vp', by='weight_decay', ax=axes[1,1])\n",
    "axes[1,1].set_title('MAE by Weight Decay')\n",
    "axes[1,1].set_xlabel('weight_decay')\n",
    "axes[1,1].set_ylabel('MAE (vp)')\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top configurations\n",
    "top_mlp = ok[ok['model_type']=='mlp'].sort_values(['mae_original','rmse_original','elbo']).head(10)\n",
    "top_conv = ok[ok['model_type']=='conv'].sort_values(['mae_original','rmse_original','elbo']).head(10)\n",
    "\n",
    "print('Top 10 MLP configs')\n",
    "display(top_mlp[['trial_name','mae_original','rmse_original','elbo','latent_dim','hidden_dims','lr','beta','batch_size','weight_decay']])\n",
    "\n",
    "print('Top 10 Conv configs')\n",
    "display(top_conv[['trial_name','mae_original','rmse_original','elbo','latent_dim','channels','fc_dim','batchnorm','lr','beta','batch_size','weight_decay']])\n",
    "\n",
    "best = ok.sort_values(['mae_original','rmse_original','elbo']).iloc[0]\n",
    "print('Overall best trial')\n",
    "display(best.to_frame().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (uv .venv)",
   "language": "python",
   "name": "thesis-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
